{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.4.0-cp37-cp37m-win_amd64.whl (370.7 MB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
      "  Using cached tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from tensorflow) (1.32.0)\n",
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.14.0-cp37-cp37m-win_amd64.whl (798 kB)\n",
      "Processing c:\\users\\zhang\\appdata\\local\\pip\\cache\\wheels\\3f\\e3\\ec\\8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\\termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Processing c:\\users\\zhang\\appdata\\local\\pip\\cache\\wheels\\62\\76\\4c\\aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\\wrapt-1.12.1-py3-none-any.whl\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorboard~=2.4\n",
      "  Using cached tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from tensorflow) (0.36.2)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.4)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (45.2.0.post20200210)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.0.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (1.5.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\zhang\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (2.2.0)\n",
      "Installing collected packages: opt-einsum, tensorflow-estimator, google-pasta, protobuf, termcolor, wrapt, tensorboard-plugin-wit, tensorboard, typing-extensions, keras-preprocessing, gast, flatbuffers, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "Successfully installed flatbuffers-1.12 gast-0.3.3 google-pasta-0.2.0 keras-preprocessing-1.1.2 opt-einsum-3.3.0 protobuf-3.14.0 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/opt-einsum/\n",
      "ERROR: astroid 2.3.3 requires typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\", which is not installed.\n",
      "ERROR: astroid 2.3.3 has requirement wrapt==1.11.*, but you'll have wrapt 1.12.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np##linear algebra\n",
    "from numpy.random import seed\n",
    "\n",
    "import pandas as pd ## data processing \n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model\n",
    "\n",
    "import tensorflow as tf #deep learning with keras\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure reproduccibility\n",
    "seed(1)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables\n",
    "\n",
    "#data path\n",
    "train_path = \"../data/train.csv\"\n",
    "test_path = \"../data/test.csv\"\n",
    "out_prediction_path = \"submission_csv\"\n",
    "\n",
    "#preprocessing\n",
    "list_col_remove = [\"Ticket\", \"Cabin\"]\n",
    "list_categ_col_to_encode = [\"Cabin_letter\", \"Cabin_first_digit\", \"Embarked\", \"Title\"]\n",
    "list_ordinal_catg_col_to_encode = [\"Fare_buckets\", \"Age_buckets\", \"Age_Class_buckets\"]\n",
    "special_remove_later = [\"Name\"]\n",
    "target_col = \"Survived\"\n",
    "components_PCA = \"full\" #'full': no PCA applied\n",
    "\n",
    "#deep learning model\n",
    "#initialization\n",
    "number_nodes_initial = 4096 #4096\n",
    "regul_L1 = 0.016 #0.02\n",
    "regul_L2 = 0.014 #0.015\n",
    "drop_out = 0.2\n",
    "\n",
    "#training \n",
    "epochs_max = 1000 #not much improvements past this point: 1000\n",
    "epoch_max_full = 5 #after training on the train set, train on hte validaiton set(to seel all data)\n",
    "patience_eval = int(round(epochs_max/1)) #change if we want to have an actual early stopping (e.g., 40)\n",
    "batch_size = 15\n",
    "checkpoint_filepath = '.mdl_wts.hdf5'\n",
    "initial_learning_param = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_train_val_predict_arrays' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-61b6ea1fe44d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#create needed arrays for training and predicting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessed_test_df\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     create_train_val_predict_arrays(train_path,test_path, list_col_remove, list_categ_col_to_encode, list_ordinal_categ_col_to_encode, \\\n\u001b[0m\u001b[0;32m      4\u001b[0m                                     special_remove_later, target_col, components_PCA, test_split_ratio)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_train_val_predict_arrays' is not defined"
     ]
    }
   ],
   "source": [
    "#create needed arrays for training and predicting\n",
    "X_train, X_test, y_train, y_test, preprocessed_test_df = \\\n",
    "    create_train_val_predict_arrays(train_path,test_path, list_col_remove, list_categ_col_to_encode, list_ordinal_categ_col_to_encode, \\\n",
    "                                    special_remove_later, target_col, components_PCA, test_split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_initialization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-e820c6720686>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m##build the training model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_initialization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_nodes_initial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregul_L1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregul_L2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_learning_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meporchs_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_learning_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_initialization' is not defined"
     ]
    }
   ],
   "source": [
    "##build the training model \n",
    "model_initialization(number_nodes_initial, X_train, regul_L1, regul_L2, drop_out, initial_learning_param)\n",
    "history, model = model_training(model, X_train, y_train, X_test, y_test, eporchs_max, patience_eval, batch_size, checkpoint_filepath, initial_learning_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##adjust model with full data(OPTIONAL)\n",
    "# X_full = np.concatenate((X_train, X_test))\n",
    "# y_full = np.concatenate((y_train, y_test))\n",
    "# model.fit(X_full, y_full, epochs = epoch_max_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-84b813b4cdc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix_results\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report_results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mrt_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s: %.2f%%\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel_metrics_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_acc' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "train_acc, test_acc, confusion_matrix_results, classification_report_results\n",
    "rt_results = model_evaluation(model, X_train, y_train, X_test, y_test)\n",
    "print(\"%s: %.2f%%\" % (model_metrics_name[1], test_acc[1]*100))\n",
    "print(confusion_matrix_results)\n",
    "print(classification_report_results)\n",
    "\n",
    "plot_history('accurary')\n",
    "# plot_history('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model\n",
    "export_pred(model, preprocessed_test_df, output_prediction_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "## preprocessing\n",
    "## pre feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df_prep, target_col, list_col_remove, n_ticket, tick_surv):\n",
    "    '''\n",
    "    combines all the functions below\n",
    "    '''\n",
    "    for function_adding_information in [cabin_information, name_information, family_information, fare_information]\n",
    "        df_prep = function_adding_information(df_prep)\n",
    "        \n",
    "    df_prep = ticket_information(df_prep, target_col, n_ticket, tick_surv)\n",
    "        #df_prep = age_information(df_prep, list_col_remove, list_categ_col_to_encode)\n",
    "        \n",
    "        #remove non-numerical remaining columns\n",
    "        df_prep = df_prep.drop(list_col_remove, axist = 1)\n",
    "        \n",
    "        return df_prep\n",
    "    \n",
    "    def cabin_inforamtion(df_prep):\n",
    "        '''\n",
    "        add first letter and first digit information wherever possible\n",
    "        '''\n",
    "        #feature to know whether the user has a cabin or not \n",
    "        df_prep[\"Has_cabin\"] = df_prep[\"Cabin\"].isna().astype(int)\n",
    "        \n",
    "        #feature to know the length of the cabin string(some users have multiple chains)-> useless\n",
    "        #df_prep[\"Canbin_length\"] = df_prep[\"Cabin\"].str.len().fillna(0)\n",
    "        \n",
    "        #first letter of the chain\n",
    "        df_prep[\"Cabin_letter\"] = df_prep[\"Cabin\"].astype(str).str[0]\n",
    "        \n",
    "        #cabin digits (first digit may mean the floor on the boat)\n",
    "        # Note, the number of digites may mean the passengers was far at the end -> Next Step\n",
    "        df_prep[\"Cabin_digits\"] = df_prep[\"Cabin\"].astype(str).str[1:]\n",
    "        \n",
    "        #get index of users who have a cabin difit after the first letter(most users who have a non_NaN value do)\n",
    "        index_with_cabin_digits = df_prep.loc[df_prep['Cabin_digits'].str.isdigit(), :].index\n",
    "        \n",
    "        #initialize the column\n",
    "        df_prep[\"Cabin_first_digit\"] = \"n\"\n",
    "        #fill in the column for the users with cabin digits\n",
    "        df_prep.loc[index_with_cabin_digits, \"Cabin_first_digit\"] = df_prep.loc[index_with_cabin_digits, 'Cabin_digits'].astype(str).str[0]\n",
    "        df_prep.drop('Cabin_digit', axis=1, inplace= True)\n",
    "        \n",
    "        return df_prep\n",
    "    \n",
    "    def name_information(df_prep):\n",
    "        '''\n",
    "        adds title information using regex from the name\n",
    "        '''\n",
    "        \n",
    "        #Create a new feature Title, containing the titles of passenger names\n",
    "        df_prep['Title'] = df_prep['Name'].apply(get_title)\n",
    "        #Group all non-common title into one single grouping \"Rare\"\n",
    "        regex_mapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Rare', 'Sir': 'Rare', 'Rev': 'Rare',\n",
    "               'Don': 'Mr', 'Mme': 'Mrs', 'Jonkheer': 'Rare', 'Lady': 'Mrs',\n",
    "               'Capt': 'Rare', 'Countess': 'Rare', 'Ms': 'Miss', 'Dona': 'Rare'}\n",
    "        df_prep.replace({'Title': regex_mapping}, inplace = True)\n",
    "        # Mapping titles\n",
    "        title_mapping  = {Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \"Master\": 3, \"Rare\": 4, \"None\":5}\n",
    "        df_prep['Title'] = df_prep['Title'].map(title_mapping)\n",
    "        df_prep['Title'] = df_prep['Title'].fillna(5)\n",
    "                          \n",
    "        return df_prep\n",
    "\n",
    "    def get_title(name):\n",
    "        title_search= re.search('([A-Za-z]+)\\.', name)\n",
    "        # if the title exists, extract and return it.\n",
    "        if title_search:\n",
    "                return title_search.group(1)\n",
    "        return \"None\"\n",
    "                          \n",
    "    def family_inforamtion(df_prep)\n",
    "        '''\n",
    "        adds 2 columns about how big the user's family is on board and whether he is alone of not\n",
    "        '''\n",
    "    # Create new features Family size as combination of SibSp adn Parch\n",
    "        df_prep['FamilySize'] = df_prep['SibSp'] + df_prep['Parch'] + 1\n",
    "    \n",
    "    # Create new features IsAlone from FamilySize\n",
    "        df_prep['isFamily'] = 1\n",
    "        df_prep.loc[df_prep['FamilySize']=1, 'isFamily'] = 0\n",
    "                          \n",
    "        return df_prep\n",
    "    \n",
    "    def fare_information(df_prep):\n",
    "        #special column for passengers who had a free tickets\n",
    "        df_prep['Fare_free'] = 0\n",
    "        index_free_fare = df_prep[df_prep['Fare']==0].index\n",
    "        df_prep.loc[index_free_fare, 'Fare_free'] = 1\n",
    "        \n",
    "        # for those 'outliners' we change the values to NaN (handled just below)\n",
    "        df_prep.loc[index_free_fare,'Fare'] = np.nan\n",
    "                          \n",
    "        #fill in missing values with median fare (not average as the ticket prices are not continuous)\n",
    "        df_prep['Fare'] = df_prep['Fare'].fillna(df_prep['Fare'].median())\n",
    "        \n",
    "        # we transfor the fare features as it doesn't follow normal distribution\n",
    "        df_prep['Fare'] = np.log(np.log(df_prep['Fare']))\n",
    "        # df_prep['Fare'] = np.log(df_prep['Fare'])\n",
    "                          \n",
    "        #Create fare buckets:\n",
    "        df_prep['Age_buckets'] = 0\n",
    "        df_prep.loc[df_prep['Age'] <=16, 'Age_buckets']=5\n",
    "        df_prep.loc[df_prep['Age'] > 16, & (df_prep['Age'] <= 32), 'Age_buckets'] = 1\n",
    "        df_prep.loc[(df_prep['Age'] > 32) & (df_prep['Age'] <= 48), 'Age_buckets'] = 2\n",
    "        df_prep.loc[(df_prep['Age'] > 48) & (df_prep['Age'] <= 64), 'Age_buckets'] = 3\n",
    "        df_prep.loc[ df_prep['Age'] > 64, 'Age_buckets'] = 4\n",
    "        \n",
    "        # add Age*Class buckets\n",
    "        df_prep['Age_Class_buckets'] = df_prep['Age_buckets']*df_prep['Pclass']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
